<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>An easier way to go: SCTP over UDP in the Linux kernel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_uKufECPE_I/easier-way-go-sctp-over-udp-linux-kernel" /><author><name>Long Xin</name></author><id>1a8d198b-2430-4971-843a-cca24741e088</id><updated>2021-06-04T07:00:00Z</updated><published>2021-06-04T07:00:00Z</published><summary type="html">&lt;p&gt;Stream Control Transmission Protocol over User Datagram Protocol (SCTP over UDP, also known as UDP encapsulation of SCTP) is a feature defined in &lt;a href="https://datatracker.ietf.org/doc/html/rfc6951"&gt;RFC6951&lt;/a&gt; and implemented in the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel space since 5.11.0. It is planned to be supported by &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8.5.0 and 9.0.&lt;/p&gt; &lt;p&gt;This article is a quick introduction to SCTP over UDP in the Linux kernel.&lt;/p&gt; &lt;h2&gt;Why we need SCTP over UDP&lt;/h2&gt; &lt;p&gt;As described in the Internet Engineering Task Force Request for Comments (RFC), there are two main reasons that we need SCTP over UDP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;To allow SCTP traffic to pass through legacy NATs, which do not provide native SCTP support as specified in [BEHAVE] and [NATSUPP]. To allow SCTP to be implemented on hosts that do not provide direct access to the IP layer. In particular, applications can use their own SCTP implementation if the operating system does not provide one.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first reason will solve the middlebox issues that have brought many troubles to users and prevented SCTP’s wide use. The second reason is to allow user space applications to develop their own SCTP implementation based on the UDP protocol.&lt;/p&gt; &lt;h2&gt;How SCTP over UDP works&lt;/h2&gt; &lt;p&gt;With this feature enabled, all SCTP packets are encapsulated into UDP packets. SCTP over UDP is implemented with kernel UDP tunnel APIs that have previously been used by the VXLAN, GENEVE, and TIPC protocols.&lt;/p&gt; &lt;p&gt;For receiving encapsulated packets, the kernel listens on one specific UDP port on all local interfaces. The default port is 9899. This port also acts as the UDP packet &lt;code&gt;src&lt;/code&gt; port for this host as a sender. As you might anticipate, the &lt;code&gt;dest&lt;/code&gt; port should be the listening port of the peer, which also defaults to 9899. The &lt;code&gt;src&lt;/code&gt; and &lt;code&gt;dest&lt;/code&gt; addresses bound to by SCTP would still be used in IP headers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | IP(v6) Header (addresses bound by SCTP) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | UDP Header (src: 9899, dest: 9899) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Common Header (SCTP ports) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Chunk #1 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | ... | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Chunk #n | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: SCTP also considers the UDP header while calculating the fragmentation point.&lt;/p&gt; &lt;h2&gt;How to use SCTP over UDP&lt;/h2&gt; &lt;p&gt;When programming, you don't need to do anything different: All the standard SCTP features still apply, and all the APIs are available to use as before. Old applications will work well without any changes or recompilation. The only adjustment is to set up a UDP port (a local listening port or &lt;code&gt;src&lt;/code&gt; port) and an &lt;em&gt;encapsulation&lt;/em&gt; port (a remote listening or &lt;code&gt;dest&lt;/code&gt; port), which could be done globally for the network namespace by &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; # sysctl -w net.sctp.encap_port=9899 # sysctl -w net.sctp.udp_port=9899&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you could set the encapsulation port per socket, association, or transport, using &lt;code&gt;sockopt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; setsockopt(SCTP_REMOTE_UDP_ENCAPS_PORT, port);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On the server side, the &lt;code&gt;encapsulation&lt;/code&gt; port normally doesn’t need to be set explicitly, as detailed in the next section.&lt;/p&gt; &lt;h2&gt;The UDP encapsulation port&lt;/h2&gt; &lt;p&gt;The UDP encapsulation port allows for very flexible usage. On the sender side, the global encapsulation port only provides a default value:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;em&gt;per-socket&lt;/em&gt; encapsulation port can be used when another socket on one host connects to a different host on which a different UDP port is used.&lt;/li&gt; &lt;li&gt;The &lt;em&gt;per-association&lt;/em&gt; encapsulation port can be used when the same socket connects to a different host on which a different UDP port is used.&lt;/li&gt; &lt;li&gt;The &lt;em&gt;per-transport&lt;/em&gt; encapsulation port can be used when the same association wants to send UDP-encapsulated SCTP packets on one transport.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;On the receiver side, the encapsulation port normally doesn’t need to be set:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The encapsulation port of one association would be learned from the first &lt;code&gt;INIT&lt;/code&gt; packet. Other &lt;code&gt;INIT&lt;/code&gt;s with different UDP &lt;code&gt;src&lt;/code&gt; ports would then be discarded.&lt;/li&gt; &lt;li&gt;The encapsulation port of each transport would be learned from the incoming packets on the corresponding path, and can be updated anytime.&lt;/li&gt; &lt;li&gt;Plain SCTP packets can still be processed even if the encapsulation ports of the association and its transports are set.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you’re using SCTP and enjoying its features, like multi-homing, multi-streaming, and partial-reliability, but having issues with middleboxes, the Linux kernel now provides an easier way to get around them.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/04/easier-way-go-sctp-over-udp-linux-kernel" title="An easier way to go: SCTP over UDP in the Linux kernel"&gt;An easier way to go: SCTP over UDP in the Linux kernel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_uKufECPE_I" height="1" width="1" alt=""/&gt;</summary><dc:creator>Long Xin</dc:creator><dc:date>2021-06-04T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/04/easier-way-go-sctp-over-udp-linux-kernel</feedburner:origLink></entry><entry><title type="html">WildFly 24 Beta1 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_4VPIIaiF9M/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/06/04/WildFly24-Beta-Released/</id><updated>2021-06-04T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 24.0.0.Beta1 releases are available for download at . Work during the WildFly 24 development cycle has been primarily oriented toward bug fixing, plus the . But I do want to express my special thanks to Sonia Zaldana, a great contributor to WildFly over the past year, who has added three new features in WildFly 24 Beta1: * * * The other area of focus during this development cycle was improving how WildFly, particularly WildFly Preview, runs on JDK 16 and the early access releases of the next LTS JDK release, JDK 17. There are still some issues to resolve, but WildFly Preview 24 Beta1 runs well enough on the latest JDKs that it’s worthwhile for people interested in what JDK 17 will mean for their application to give it a look. The release notes for the release are , with issues fixed in the underlying WildFly Core betas listed . Please try it out and give us your feedback, while we get to work WildFly 24 Final! Best regards, Brian&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_4VPIIaiF9M" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/06/04/WildFly24-Beta-Released/</feedburner:origLink></entry><entry><title>DevNation Deep Dive: Argo CD</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NWxqEzgkyEY/devnation-deep-dive-argo-cd" /><author><name>Natale Vinto</name></author><id>3aa32954-3e4f-4594-9dcf-227e5e9a78a7</id><updated>2021-06-03T15:24:28Z</updated><published>2021-06-03T15:24:28Z</published><summary type="html">&lt;p&gt;Argo CD is a declarative &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous delivery&lt;/a&gt; tool for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. It follows the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; pattern of using Git repositories as the source of truth for defining the desired application state.&lt;/p&gt; &lt;p&gt;Argo CD automates the deployment of desired application states in specified target environments. Application deployments can track updates to branches, tags, or pin to a specific version of manifests at a Git commit.&lt;/p&gt; &lt;h2&gt;Take a deep dive into Argo CD and GitOps&lt;/h2&gt; &lt;p&gt;Learn the basics of Argo CD and GitOps in this &lt;a href="https://developers.redhat.com/devnation/deep-dive"&gt;DevNation Deep Dive&lt;/a&gt;. Courses are offered in five languages (English, Italian, Spanish, French, and Brazilian Portuguese) across multiple time zones.&lt;/p&gt; &lt;p&gt;The Argo CD Deep Dive will cover:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The fundamentals of GitOps and Argo CD&lt;/li&gt; &lt;li&gt;How to install and manage Argo CD in Kubernetes and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deploying and syncing applications&lt;/li&gt; &lt;li&gt;Using Kustomize with Argo CD&lt;/li&gt; &lt;li&gt;Using sync waves and hooks with Argo CD&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Sessions begin June 8. &lt;a href="https://developers.redhat.com/devnation/deep-dive/argocd"&gt;Register now!&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/03/devnation-deep-dive-argo-cd" title="DevNation Deep Dive: Argo CD"&gt;DevNation Deep Dive: Argo CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NWxqEzgkyEY" height="1" width="1" alt=""/&gt;</summary><dc:creator>Natale Vinto</dc:creator><dc:date>2021-06-03T15:24:28Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/03/devnation-deep-dive-argo-cd</feedburner:origLink></entry><entry><title type="html">New DMN Boxed Expressions Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/X-6gRthc7Ec/new-dmn-boxed-expression-editor.html" /><author><name>Valentino Pellegrino</name></author><id>https://blog.kie.org/2021/06/new-dmn-boxed-expression-editor.html</id><updated>2021-06-03T07:39:00Z</updated><content type="html">While working with the DMN editor, you may have noticed that a big amount of modeling logic lives on the expressions, boxed in a few kinds of DMN nodes.  The Boxed Expressions are crucial for modeling decision logic. On that side, there is an ongoing effort from the engineering team to improve the user experience of the Boxed Expression Editor, which will be described in a series of articles that will clarify reasons, solutions, and the next steps. This first article will briefly explore Boxed Expressions from their perspective, looking at how they are currently leveraged in the DMN editor. WHAT ARE BOXED EXPRESSIONS A Decision Requirements Diagram (DRD) is a graphical representation of the decision model structure. In such a diagram, you can define decision nodes and each of them has a value expression (or decision logic) that determines its output based on its current inputs. The logic defined in such expressions is expressed in tabular formats called boxed expressions. The DMN editor supports a variety of decision logic types: * Decision Table: it is the most familiar type of boxed expression. Its logic is based on rules. It is basically a table where the columns on the left represent the inputs, and the column on the right, instead, is the output. Each row is a decision rule. If all cells belonging to the input columns of a decision rule match the input data, then the output of the decision rule gets selected. On the top-left corner of the table, there is a hit policy, which specifies what the result of the Decision Table is in cases of overlapping rules. Below, a possible layout for the Decision Table, taken directly from the spec. * Literal Expression: a single cell that represents a formula, using the expression language. Below, an example of Literal Expression. * Invocation: it’s an expression that maps inputs to parameterized decision logic. In the example below, the invoked decision logic is a business knowledge model. * List: a collection of items. When cells are arranged vertically, the layout is similar to: * Context: a collection of entries (name, value) with an optional result value. It can have nested values. The context when it is in a vertical layout looks like the following: * Relation: it’s a table representing only values. Quite simple. A collection of columns and rows represent a Relation: * Function: it’s a notation for parameterized boxed expressions. It has three cells: the function kind, the list of parameters, and the function body. Below you can find a Function layout: BOXED EXPRESSIONS IN THE CURRENT DMN EDITOR I am sure you have already used the DMN Editor to edit a Boxed Expression. By the way, let’s take one step at a time. First of all, let’s go to the well-known website. Here, there is a ready-to-use online version of the DMN editor: You have two ways for editing a Boxed Expression: 1. Edit a Business Knowledge Model node. In this case, you are able to edit only its Function definition. 2. Edit a Decision Service node. In this way, it is possible to specify the desired expression logic type by using a selector. Editor look &amp;amp; feel is very close to what the spec says.  Let’s take, as an example, the Literal Expression, which is the most minimal logic type.  You can navigate back to the DMN Diagram by clicking the link on the top. The section below contains both the decision node name and the selected logic type. On the bottom, we have the boxed expression itself, with the header containing the value expression’s name and type ref, and a body containing the text field, that you can fill with FEEL code. I suggest you play with the editor, trying to select other logic types. For example, what happens if you need to represent a Context table, where one entry maps to a Literal Expression, and another maps to a List of Literal Expressions? Well, you can find out that there are several business cases where it is necessary to specify nested expressions and we have few logic types that suit such cases well. CONCLUSION We have covered just a few topics related to Boxed Expressions. The main goal here was to communicate how crucial they are for the DMN ecosystem. You will find more information about DMN modeling and related use-cases on the official . The existing Boxed Expressions editor already looks great, so what may you expect from the new one? The answer will be part of the next article! We will talk about the state of the art for this editor, exploring rooms for enhancements, and how we’re improving the overall user experience in the new one. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/X-6gRthc7Ec" height="1" width="1" alt=""/&gt;</content><dc:creator>Valentino Pellegrino</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/new-dmn-boxed-expression-editor.html</feedburner:origLink></entry><entry><title>Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_mxUp6SVqAE/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally" /><author><name>Brian Gollaher</name></author><id>3ff7100c-f6c1-4a6f-b8de-03b2443f0f71</id><updated>2021-06-03T07:00:00Z</updated><published>2021-06-03T07:00:00Z</published><summary type="html">&lt;p&gt;The latest versions of &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; and &lt;a href=" /products/developertoolset/overview"&gt;Red Hat Developer Toolset&lt;/a&gt; are now generally available. Red Hat Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;world’s leading enterprise Linux platform&lt;/a&gt;. These components are supported for up to five years for a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;h2&gt;What's new in Red Hat Software Collections 3.7&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;: This version provides a number of new features and enhancements over version 12. Notable changes include performance improvements resulting from de-duplication of B-tree index entries, improved performance for queries that use aggregates or partitioned tables, improved query planning when using extended statistics, parallelized vacuuming of indexes, and incremental sorting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;: Notable enhancements over the previously available version 10.3 include a number of security updates, new features, and updates to the InnoDB storage engine. In addition, the MariaDB Galera Cluster has been upgraded to version 4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ruby 3.0&lt;/strong&gt;: This version provides a number of bug fixes and enhancements over the previously released Ruby 2.7. Being a new major version, it brings speed improvements, introduces language to describe the types (RBS), and provides concurrent abstraction via Ractor. The new version now also separates keyword arguments from other arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Java Mission Control 8.0.0 (update)&lt;/strong&gt;: This is an advanced set of tools for managing, monitoring, profiling, and troubleshooting Java applications. Updates to Java Mission Control include adding new graphs and viewers for stack traces, threads, and memory usage.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Developer Toolset 10.1&lt;/h2&gt; &lt;p&gt;Also new in Red Hat Software Collections 3.7 is Developer Toolset 10.1, which features GNU Compiler Collection (GCC) 10, an updated, curated collection of compilers, toolchains, debuggers, and other critical development tools. Forming the foundation of Developer Toolset 10 is GCC 10.2.1, a new update of the popular open source compiler collection. Additional updates in Developer Toolset 10.1 center on delivering new updates of &lt;a href="https://developers.redhat.com/topics/c"&gt;C/C++&lt;/a&gt; and Fortran debugging and &lt;a href="https://developers.redhat.com/blog/category/performance/"&gt;performance&lt;/a&gt; tools.&lt;/p&gt; &lt;p&gt;All new collections in Red Hat Software Collections 3.7 are also available as &lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;Red Hat Certified Containers&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt; Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy applications using the supported components of Red Hat Software Collections for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;Red Hat Software Collections 3.7 continues Red Hat’s commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Red Hat customers with active &lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Red Hat Software Collections via the &lt;a href="https://access.redhat.com/solutions/472793"&gt;Red Hat Software Collections repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information, please read the full &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/03/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally" title="Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_mxUp6SVqAE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-06-03T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/03/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally</feedburner:origLink></entry><entry><title>Simulating CloudEvents with AsyncAPI and Microcks</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CKaov6paM0c/simulating-cloudevents-asyncapi-and-microcks" /><author><name>Laurent Broudoux</name></author><id>6b56bdb0-3134-4971-9897-3d172f4ea6ad</id><updated>2021-06-02T07:00:00Z</updated><published>2021-06-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/event-driven"&gt;Event-driven architecture&lt;/a&gt; was an evolutionary step toward cloud-native applications, and supports &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications. Events connect &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, letting you decouple functions in space and time and make your applications more resilient and elastic.&lt;/p&gt; &lt;p&gt;But events come with challenges. One of the first challenges for a development team is how to describe events in a repeatable, structured form. Another challenge is how to work on applications that consume events without having to wait for another team to hand you the applications that produce those events.&lt;/p&gt; &lt;p&gt;This article explores those two challenges and shows how to simulate events using &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt;, &lt;a href="https://asyncapi.com"&gt;AsyncAPI&lt;/a&gt;, and &lt;a href="https://microcks.io"&gt;Microcks&lt;/a&gt;. CloudEvents and AsyncAPI are complementary specifications that you can combine to help define an event-driven architecture. Microcks allows simulation of CloudEvents to speed up and protect the autonomy of development teams.&lt;/p&gt; &lt;h2&gt;CloudEvents or AsyncAPI?&lt;/h2&gt; &lt;p&gt;New standards such as &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; or &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt; have emerged to address the need to describe events in a structured format. People often ask: "Should I use CloudEvents or AsyncAPI?" There's a widespread belief that CloudEvents and AsyncAPI compete within the same scope. I see things differently, and in this article, I'll explain how the two standards work well together.&lt;/p&gt; &lt;h3&gt;What is CloudEvents?&lt;/h3&gt; &lt;p&gt;The essence of CloudEvents can be found on a statement from &lt;a href="http://cloudevents.io/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;CloudEvents is a specification for describing event data in common formats to provide interoperability across services, platforms, and systems.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The purpose of CloudEvents is to establish a common format for event data description. CloudEvents is part of the Cloud Native Computing Foundation's &lt;a href="https://github.com/cncf/wg-serverless"&gt; Serverless Working Group&lt;/a&gt;. A lot of integrations already exist within &lt;a href="https://knative.dev/docs/eventing/"&gt;Knative Eventing&lt;/a&gt; (or &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;), &lt;a href="https://www.triggermesh.com/"&gt;Trigger Mesh&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/services/event-grid"&gt;Azure Event Grid&lt;/a&gt;, allowing true cross-vendor platform interoperability.&lt;/p&gt; &lt;p&gt;The CloudEvents specification is focused on events and defines a common envelope (set of attributes) for your application event. As of today, CloudEvents proposes two different &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#13-content-modes"&gt;content modes&lt;/a&gt; for transferring events: structured and binary.&lt;/p&gt; &lt;p&gt;The CloudEvents repository offers an &lt;a href="https://gist.github.com/e3261b13eb7a9dbb14ccf59b1580d5b7#file-cloudevent-json"&gt;example of a JSON structure containing event attributes&lt;/a&gt;. This is a &lt;em&gt;structured CloudEvent&lt;/em&gt;. The event data in the example is XML, contained in the value &lt;code&gt;&lt;much wow=\"xml\"/&gt;&lt;/code&gt;, but it can be of any type. CloudEvents takes care of defining meta-information about your event, but does not help you define the actual event content:&lt;/p&gt; &lt;pre&gt; { "specversion" : "1.0.1", "type" : "com.github.pull.create", "source" : "https://github.com/cloudevents/spec/pull/123", "id" : "A234-1234-1234", "time" : "2020-04-05T17:31:00Z", "comexampleextension1" : "value", "comexampleextension2" : { "othervalue": 5 }, "contenttype" : "text/xml", "data" : "&lt;much wow=\"xml\"/&gt;" } &lt;/pre&gt; &lt;h3&gt;What is AsyncAPI?&lt;/h3&gt; &lt;p&gt;To understand AsyncAPI, again we turn to &lt;a href="http://asyncapi.com/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;AsyncAPI is an industry standard for defining asynchronous APIs. Our long-term goal is to make working with EDAs as easy as it is to work with REST APIs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The "API" stands for &lt;a href="https://developers.redhat.com/topics/api-management"&gt;application programming interface&lt;/a&gt; and embodies an application's interactions and capabilities. AsyncAPI can be seen as the sister specification of &lt;a href="https://www.openapis.org/"&gt;OpenAPI&lt;/a&gt;, but targeting asynchronous protocols based on event brokering.&lt;/p&gt; &lt;p&gt;AsyncAPI focuses on the application and the communication channels it uses. Unlike CloudEvents, AsyncAPI does not define how your events should be structured. However, AsyncAPI provides an extended means to precisely define both the meta-information and the actual content of an event.&lt;/p&gt; &lt;p&gt;An &lt;a href="https://gist.github.com/67252933bcfea50c996b44dd20225962#file-asyncapi-yml"&gt;example in YAML&lt;/a&gt; can be found on GitHub. This example describes an event with the title &lt;code&gt;User signed-up event&lt;/code&gt;, published to the &lt;code&gt;user/signedup&lt;/code&gt; channel. These events have three properties: &lt;code&gt;fullName&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; . Each property is defined using semantics from &lt;a href="https://json-schema.org/"&gt;JSON Schema&lt;/a&gt;. Although it's not shown in this example, AsyncAPI also allows you to specify event headers and whether these events will be available through different protocol bindings such as &lt;a href="https://kafka.apache.org/"&gt;Kafka&lt;/a&gt;, &lt;a href="https://www.amqp.org/"&gt;AMQP&lt;/a&gt;, &lt;a href="https://mqtt.org/"&gt;MQTT&lt;/a&gt;, or &lt;a href="https://www.w3.org/TR/websockets/"&gt;WebSocket&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: 2.0.0 id: urn:com.asyncapi.examples.user info: title: User signed-up event version: 0.1.1 channels: user/signedup: publish: message: payload: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;CloudEvents with AsyncAPI&lt;/h2&gt; &lt;p&gt;The explanations and examples I've shown reveal that the CloudEvents with AsyncAPI standards tackle different scopes. Thus they do not have to be treated as mutually exclusive. You can actually combine them to achieve a complete event specification, including application definition, channels description, structured envelope, and detailed functional data carried by the event.&lt;/p&gt; &lt;p&gt;The general idea behind the combination is to use an AsyncAPI specification as a hosting document. It holds references to CloudEvents attributes and adds more details about the event format.&lt;/p&gt; &lt;p&gt;You can use two mechanisms in AsyncAPI to ensure this combination. Choosing the correct mechanism might depend on the protocol you choose to convey your events. Things aren't perfect yet and you'll have to make a choice.&lt;/p&gt; &lt;p&gt;Let's take the example of using &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; to distribute events:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;In the structured content mode, CloudEvents meta-information is tangled up with the &lt;code&gt;data&lt;/code&gt; in the messages value. For that mode, we'll use the JSON Schema composition mechanism that is accessible from AsyncAPI.&lt;/li&gt; &lt;li&gt;In the binary content mode (for which we can use &lt;a href="https://avro.apache.org/"&gt;Avro&lt;/a&gt;), meta-information for each event is dissociated from the message value and inserted, instead, into the header of each message. For that, we'll use the &lt;a href="https://www.asyncapi.com/docs/specifications/2.0.0#messageTraitObject"&gt;MessageTrait&lt;/a&gt; application mechanism present in AsyncAPI.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Structured content mode&lt;/h3&gt; &lt;p&gt;This section rewrites the previous AsyncAPI example to use CloudEvents in structured content mode. The resulting &lt;a href="https://gist.github.com/035ccc4d7b7cdd414f0ebc5a53e80c4c#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; contains the following elements worth noting:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The definition of &lt;code&gt;headers&lt;/code&gt; that starts on line 16 contains our application's &lt;code&gt;custom-header&lt;/code&gt;, as well as the mandatory CloudEvents &lt;code&gt;content-type&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;schemas&lt;/code&gt; field refers to the CloudEvents specification on line 33, re-using this specification as a basis for our message.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;schemas&lt;/code&gt; field also refers to a refined version of the &lt;code&gt;data&lt;/code&gt; property description on line 36.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API structured version: 0.1.3 defaultContentType: application/json channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 headers: type: object properties: custom-header: type: string content-type: type: string enum: - 'application/cloudevents+json; charset=UTF-8' payload: $ref: '#/components/schemas/userSignedUpPayload' examples: [...] components: schemas: userSignedUpPayload: type: object allOf: - $ref: 'https://raw.githubusercontent.com/cloudevents/spec/v1.0.1/spec.json' properties: data: $ref: '#/components/schemas/userSignedUpData' userSignedUpData: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Binary content mode&lt;/h3&gt; &lt;p&gt;Now, we'll apply the binary content mode to the AsyncAPI format. The resulting &lt;a href="https://gist.github.com/d5eca1c76fd57e5b3326b5d5db26bbd3#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; shows that event properties have moved out of this format. Other important things to notice here are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A trait is applied at the message level on line 16. The trait resource is a partial AsyncAPI document containing a &lt;code&gt;MessageTrait&lt;/code&gt; definition. This trait will bring in all the mandatory attributes (&lt;code&gt;ce_*&lt;/code&gt;) from CloudEvents. It is the equivalent of the CloudEvents JSON Schema.&lt;/li&gt; &lt;li&gt;This time we're specifying our event payload using an Avro schema as specified on line 25.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: [...] &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;What are the benefits of combining CloudEvents with AsyncAPI?&lt;/h3&gt; &lt;p&gt;Whichever content mode you chose, you now have a comprehensive description of your event and all the elements of your event-driven architecture. The description guarantees the low-level interoperability of the CloudEvents-plus-AsyncAPI combination, along with the ability to be routed and trigger a function in a serverless world. In addition, you provide a complete description of the carried &lt;code&gt;data&lt;/code&gt; that will be of great help for applications consuming and processing events.&lt;/p&gt; &lt;h2&gt;Simulating CloudEvents with Microcks&lt;/h2&gt; &lt;p&gt;Let's tackle the second challenge stated at the beginning of this article: How can developers efficiently work as a team without having to wait for someone else's events? We've seen how to fully describe events. However, it would be even better to have a pragmatic approach for leveraging this CloudEvents-plus-AsyncAPI contract. That's where &lt;a href="https://microcks.io/"&gt;Microcks&lt;/a&gt; comes to the rescue.&lt;/p&gt; &lt;h3&gt;What is Microcks?&lt;/h3&gt; &lt;p&gt;Microcks is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-native tool for mocking/simulating and testing APIs. One purpose of Microcks is to turn your API contract (such as OpenAPI, AsyncAPI, or the &lt;a href="https://getpostman.com/"&gt;Postman&lt;/a&gt; collection) into live mocks in seconds. Once it has imported your AsyncAPI contract, Microcks starts producing mock events on a message broker at a defined frequency.&lt;/p&gt; &lt;p&gt;Using Microcks you can simulate CloudEvents in seconds, without writing a single line of code. Microcks allows the team that is relying on input events to immediately start working. They do not have to wait for the team that is coding the application that will publicize events.&lt;/p&gt; &lt;h3&gt;Using Microcks for CloudEvents&lt;/h3&gt; &lt;p&gt;To produce CloudEvents events with Microcks, simply re-use examples by adding them to your contract. We omitted the &lt;code&gt;examples&lt;/code&gt; property before, but we'll now add that property to our &lt;a href="https://gist.github.com/820c925b8ff84929ebf0c30ad1900c62#file-asyncapi-ce-yaml"&gt;example in binary content mode&lt;/a&gt; on line 26:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: - john: summary: Example for John Doe user headers: ce_specversion: "1.0" ce_type: "io.microcks.example.user-signedup" ce_source: "/mycontext/subcontext" ce_id: "{{uuid()}}" ce_time: "{{now(yyyy-MM-dd'T'HH:mm:SS'Z')}}" content-type: application/avro sentAt: "2020-03-11T08:03:38Z" payload: fullName: John Doe email: john@microcks.io age: 36&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key points to note are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can put in as many examples as you want because this property becomes a map in AsyncAPI.&lt;/li&gt; &lt;li&gt;You can specify both &lt;code&gt;headers&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt; values.&lt;/li&gt; &lt;li&gt;Even if &lt;code&gt;payload&lt;/code&gt; will be Avro-binary encoded, you use YAML or JSON to specify examples.&lt;/li&gt; &lt;li&gt;You can use templating functions through the &lt;code&gt;{{ }}&lt;/code&gt; notation to introduce &lt;a href="https://microcks.io/documentation/using/advanced/templates/#function-expressions"&gt;random or dynamic values&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Once the schema is imported into Microcks, it discovers the API definition as well as the different examples. Microcks starts immediately producing mock events on the Kafka broker it is connected to—every three seconds in our example (see Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-simulating-cloud-events_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-simulating-cloud-events_0.png?itok=uI0qn302" width="600" height="327" alt="A Microcks import of AsyncAPI with CloudEvents." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Microcks import of AsyncAPI with CloudEvents. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since release &lt;a href="https://microcks.io/blog/microcks-1.2.0-release/"&gt;1.2.0&lt;/a&gt;, Microcks also supports the connection to a schema registry. Therefore, it publishes the Avro schema used at the mock-message publication time. You can use the &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; command-line interface (CLI) tool to connect to the Kafka broker and registry, and then inspect the content of mock events. Here we're using the &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio service registry&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ kafkacat -b my-cluster-kafka-bootstrap.apps.try.microcks.io:9092 -t UsersignedupCloudEventsAPI_0.1.3_user-signedup -s value=avro -r http://apicurio-registry.apps.try.microcks.io/api/ccompat -o end -f 'Headers: %h - Value: %s\n' --- OUTPUT % Auto-selecting Consumer mode (use -P or -C to override) % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 276 Headers: sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_id=7a8cc388-5bfb-42f7-8361-0efb4ce75c20,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_time=2021-03-09T15:17:762Z,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 277 Headers: ce_id=dde8aa04-2591-4144-aa5b-f0608612b8c5,sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_time=2021-03-09T15:17:733Z,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 279&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can check that the emitted events respect both the CloudEvents meta-information structure and the AsyncAPI &lt;code&gt;data&lt;/code&gt; definition. Moreover, each event has different random attributes, which allows it to simulate diversity and variation for the consuming application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how to solve some of the challenges that come with an event-driven architecture.&lt;/p&gt; &lt;p&gt;First, I described how recent standards, CloudEvents and AsyncAPI, focus on different scopes: the event for CloudEvents and the application for AsyncAPI.&lt;/p&gt; &lt;p&gt;Then I demonstrated how to combine the specifications to provide a comprehensive description of all the elements involved in an event-driven architecture: application definition, channels description, structured envelope, and detailed functional data carried by the event. The specifications are complementary, so you can use one or both depending on how deep you want to go in your formal description.&lt;/p&gt; &lt;p&gt;Finally, you've seen how to use Microcks to simulate any events based on AsyncAPI, including those generated by CloudEvents, just by using examples. Microcks answers the challenge of working, testing, and validating autonomously when different development teams are using an event-driven architecture.&lt;/p&gt; &lt;p&gt;I hope you learned something new—if so, please consider reacting, commenting, or sharing. Thanks for reading.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks" title="Simulating CloudEvents with AsyncAPI and Microcks"&gt;Simulating CloudEvents with AsyncAPI and Microcks&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CKaov6paM0c" height="1" width="1" alt=""/&gt;</summary><dc:creator>Laurent Broudoux</dc:creator><dc:date>2021-06-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks</feedburner:origLink></entry><entry><title type="html">Custom Layered Immutable Spring Boot Kie Server</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hYH2jJwUYUQ/custom-layered-immutable-spring-boot-kie-server.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</id><updated>2021-06-01T23:06:08Z</updated><content type="html">“Uncommon thinkers reuse what common thinkers refuse” (J.R.D. Tata) When creating an image for an immutable Spring Boot Kie Server application, it is possible to split up the files and folders belonging to the fat-jar into different layers. Main advantages of this approach are: 1. Libraries, code, and resources are grouped into layers based on the likelihood to change between builds. This reduces the image generation time. 2. Layers downloaded once (saving disk space and bandwidth) and reused for other images. 3. Execution over the unzipped classes is a little bit faster than launching the fat-jar: java -jar app.jar Considering all these points, it is completely meaningful to layer the immutable Spring Boot Kie Server -isolating the KJARs into a new custom layer- as business assets in KJARs are more likely to change. PACKAGING SPRING BOOT KIE SERVER WITH LAYERS The spring-boot-maven-plugin is in charge of creating the immutable fat-jar containing all the KJAR files and their dependencies. For triggering this process, just add the following properties to the Spring Boot application.properties file: kieserver.classPathContainer=true kieserver.autoScanDeployments=true Next, we have to enable the layers into the pom.xml, pointing out to the configuration file layers.xml where we define how the folders, files, and resources are separated into different layers and the order of them. &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${springboot.version}&lt;/version&gt; &lt;configuration&gt; &lt;layers&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;configuration&gt;${project.basedir}/src/layers.xml&lt;/configuration&gt; &lt;/layers&gt; &lt;image&gt; &lt;name&gt;${spring-boot.build-image.name}&lt;/name&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; In this layers.xml file, we define the following layers in this order (the first four are default ones, adding custom kjars layer at the end, as it is the more likely to change during application lifetime): * dependencies any dependency whose version does not contain SNAPSHOT. * spring-boot-loader for the loader classes. * snapshot-dependencies dependencies whose version contains SNAPSHOT. * application for local module dependencies, application classes, and resources but KJARs. * kjars for all the KJARs in the BOOT-INF/classes/KIE-INF folder, which were separated during package-dependencies-kjar goal execution. &lt;layers xmlns="http://www.springframework.org/schema/boot/layers" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/boot/layers https://www.springframework.org/schema/boot/layers/layers-2.5.xsd"&gt; &lt;application&gt; &lt;into layer="spring-boot-loader"&gt; &lt;include&gt;org/springframework/boot/loader/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="kjars"&gt; &lt;include&gt;BOOT-INF/classes/KIE-INF/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="application" /&gt; &lt;/application&gt; &lt;dependencies&gt; &lt;into layer="snapshot-dependencies"&gt; &lt;include&gt;*:*:*SNAPSHOT&lt;/include&gt; &lt;/into&gt; &lt;into layer="dependencies"&gt; &lt;includeModuleDependencies /&gt; &lt;/into&gt; &lt;/dependencies&gt; &lt;layerOrder&gt; &lt;layer&gt;dependencies&lt;/layer&gt; &lt;layer&gt;spring-boot-loader&lt;/layer&gt; &lt;layer&gt;snapshot-dependencies&lt;/layer&gt; &lt;layer&gt;application&lt;/layer&gt; &lt;layer&gt;kjars&lt;/layer&gt; &lt;/layerOrder&gt; &lt;/layers&gt; Finally, once we have the configuration of the pom.xml and layers.xml set up, we may launch the package process by invoking to the maven command: $ mvn clean package -DskipTests SPRING BOOT KIE SERVER LAYERS INSPECT We can check out the result of this layering process using the property jarmode=layertools with the list argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar list dependencies spring-boot-loader snapshot-dependencies application kjars Our custom kjars layer is the last one as defined in the layerOrder node of layers.xml file. Another interesting file we may check out is the layers.idx where packaging information (separated folders and layer order) is stored. $ cat application/BOOT-INF/layers.idx - "dependencies": - "BOOT-INF/lib/" - "spring-boot-loader": - "org/" - "snapshot-dependencies": - "application": - "BOOT-INF/classes/application.properties" - "BOOT-INF/classes/org/" - "BOOT-INF/classes/quartz-db.properties" - "BOOT-INF/classpath.idx" - "BOOT-INF/layers.idx" - "META-INF/" - "kjars": - "BOOT-INF/classes/KIE-INF/" Moreover, we can extract the layers again using the property jarmode=layertools with the extract argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar extract $ tree kjars kjars └── BOOT-INF └── classes └── KIE-INF └── lib ├── kjar-sample-1.0.0.jar ├── kjar-sample-1.1.0.jar └── other-kjar-1.0.0.jar 4 directories, 3 files We can use this utility in the Dockerfile for easily extracting the layers of the fat-jar and dockerize our immutable Spring Boot Kie Server application. LAYERED DOCKERFILE We can define a multi-stage Dockerfile to take advantage of this layering and build the image: FROM openjdk:8-slim as builder WORKDIR application ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} immutable-springboot-kie-server-1.0.0.jar RUN java -Djarmode=layertools -jar immutable-springboot-kie-server-1.0.0.jar extract FROM openjdk:8-slim WORKDIR application COPY --from=builder application/dependencies/ ./ COPY --from=builder application/spring-boot-loader/ ./ COPY --from=builder application/snapshot-dependencies/ ./ COPY --from=builder application/application/ ./ COPY --from=builder application/kjars/ ./ ENTRYPOINT ["java", "org.springframework.boot.loader.JarLauncher"] TIP: You may consider any other layering depending on the likelihood of changes for the grouped libraries, code, and resources. Happy layering!! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hYH2jJwUYUQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</feedburner:origLink></entry><entry><title>Join the Red Hat team at OpenJS World 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/a2xNf6CngUk/join-red-hat-team-openjs-world-2021" /><author><name>Michael Dawson</name></author><id>066f25a4-7a0e-43d1-be62-1bfd9f4ef265</id><updated>2021-06-01T12:30:00Z</updated><published>2021-06-01T12:30:00Z</published><summary type="html">&lt;p&gt;Red Hat is excited to be back at the &lt;a href="https://openjsf.org/openjs-world-2021/"&gt;OpenJS World&lt;/a&gt; conference again this year. We look forward to connecting with you to explore the impact &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; are having on technologies of all kinds, especially in the area of cloud-native development.&lt;/p&gt; &lt;p&gt;Many developers and community contributors from the Red Hat and IBM teams will deliver talks and participate in conference events. We hope to see you there!&lt;/p&gt; &lt;h2&gt;OpenJS World keynotes, sessions, and labs hosted by Red Hat and IBM&lt;/h2&gt; &lt;p&gt;This year's virtual event includes many great topics and speakers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keynotes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Wednesday, June 2 at 9:00 a.m. PDT: &lt;a href="https://openjsworld2021.sched.com/event/j00p/welcome-keynote-the-roaring-twenties-for-javascript-robin-bender-ginn-executive-director-openjs-foundation-todd-moore-vp-of-open-technology-and-developer-advocacy-ibm"&gt;The Roaring Twenties for JavaScript&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Todd Moore (Vice President, Open Technology and Developer Advocacy, IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Wednesday, June 2 at 10:25 a.m. PDT: &lt;a href="https://openjsworld2021.sched.com/event/j06C/keynote-open-open-source-and-making-great-places-for-collaboration-joe-sepi-open-source-engineer-advcoate-ibm-michael-dawson-nodejs-lead-for-ibm-and-red-hat-beth-griggs-senior-software-engineer-red-hat"&gt;Open Open Source and Making Great Places for Collaboration&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Joe Sepi (Open Source Engineer &amp; Advocate, IBM), Michael Dawson (Node.js lead, IBM and Red Hat), and Bethany Griggs (Senior Software Engineer, Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Sessions:&lt;/strong&gt; Available together June 2 at 1:00 p.m. PDT.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzz/internet-of-things-iot-with-node-both-practical-and-fun-jesse-gorzinski-ibm-michael-dawson-red-hat"&gt;Internet of Things (IoT) with Node: Both Practical and Fun!&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Jesse Gorzinski (IBM) and Michael Dawson (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j002/nodejs-diagnostic-best-practices-gireesh-punathil-ibm-india-mary-marchini-netflix"&gt;Node.js Diagnostic Best Practices&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Gireesh Punathil (IBM India)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izze/panel-nodejs-package-maintenance-working-group-year-3-glenn-hinks-american-express-bethany-griggs-red-hat-darcy-clarke-github-dominykas-blyze-nearform-rodion-abdurakhimov-aspire-global"&gt;Node.js Package Maintenance Working Group: Year 3&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Panel discussion featuring Bethany Griggs (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j06f/responsible-coding-for-a-better-future-lucile-jerber-stephane-rodet-ibm"&gt;Responsible Coding for a Better Future&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Lucile Jerber and Stephane Rodet (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j00K/take-a-trip-through-jslandia-joe-sepi-ibm-jory-burson-linux-foundation"&gt;Take a Trip through JSLandia&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Joe Sepi (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j005/application-modernization-with-camel-javascript-and-openshift-ip-sam-wuxin-zeng-red-hat"&gt;Application Modernization with Camel JavaScript and OpenShift&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Ip Sam and Wuxin Zeng (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzt/nodejs-deep-debugging-gireesh-punathil-ibm-india"&gt;Node.js Deep Debugging&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Gireesh Punathil (IBM India)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzD/nodejs-the-new-and-the-experimental-bethany-griggs-red-hat"&gt;Node.js: The New and the Experimental&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Bethany Griggs (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izyR/cloud-native-landscape-for-nodejs-developers-upkar-lidder-ibm"&gt;Cloud Native Landscape for Node.js Developers&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Upkar Lidder (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We are also hosting a set of labs on June 3. For more information, check out &lt;a href="https://developer.ibm.com/conferences/openjs-world/"&gt;OpenJS World: IBM Day of Workshops&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Explore more Node.js resources&lt;/h2&gt; &lt;p&gt;If you want to learn more about Red Hat and IBM’s involvement in the Node.js community and what we are working on, check out our topic pages at &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Red Hat Developer&lt;/a&gt; and &lt;a href="https://developer.ibm.com/languages/node-js/"&gt;IBM Developer&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/01/join-red-hat-team-openjs-world-2021" title="Join the Red Hat team at OpenJS World 2021"&gt;Join the Red Hat team at OpenJS World 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/a2xNf6CngUk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2021-06-01T12:30:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/01/join-red-hat-team-openjs-world-2021</feedburner:origLink></entry><entry><title>How to create a better front-end developer experience</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lVMpKj-n8rc/how-create-better-front-end-developer-experience" /><author><name>Zackary Allen</name></author><id>9a17ebc5-2eca-4ea4-bdeb-3443661d9df2</id><updated>2021-06-01T07:00:00Z</updated><published>2021-06-01T07:00:00Z</published><summary type="html">&lt;p&gt;Who are the first users of a new feature or new application? If you think they are customers, think again.&lt;/p&gt; &lt;p&gt;The first users are actually the front-end developers, and their experience testing those new applications and features makes your first &lt;a href="https://developers.redhat.com/blog/category/uiux/"&gt;user experience&lt;/a&gt; (UX). If your front-end developers have a smooth experience developing new products, your users will almost always have a smooth experience using them.&lt;/p&gt; &lt;p&gt;Take developing a form using React, for example. If developers are able to develop the form without any difficulty, it will likely be a positive experience for the customer as well. The reason? The developer had to fill out the form to test it. If tweaking the form takes one second but filling it out takes one minute, the developer will probably find a way to reduce the feedback loop. It might be reduced through technical means by integrating with browsers that autofill address fields, or by advising the design team that the form could be split up so it can be more modularly tweaked and tested. Whatever the case, &lt;em&gt;developers tend to write software consistent with their tools&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;That's why UX design teams should strive to not only improve the end-user experience in their products, but also streamline the experience of the developers who build them. &lt;em&gt;Developer experience&lt;/em&gt; refers to the workflow developers move through as they write, update, and maintain code for each release. From local build tooling to shared workflows and shared deployments, strong developer experience paves the path for solid UX.&lt;/p&gt; &lt;p&gt;In this article, we'll explore common pain points that can complicate the development process and how to address them to foster better developer experiences.&lt;/p&gt; &lt;h2&gt;What makes a good developer experience?&lt;/h2&gt; &lt;p&gt;Front-end developers want to be able to write code to add features in an environment that closely resembles what users will encounter in the end product. After committing and pushing their code changes, they'd typically like to run tests to make sure their changes don't break anything unexpectedly. Beyond test validation, front-end developers might want to validate new features by sharing a link with stakeholders. Once their changes meet the stakeholders' criteria, developers want their code to make its way into the central repository and then to end users. Sometimes the new features reach end users as soon as the changes are merged, and sometimes that transition happens on a time-based schedule.&lt;/p&gt; &lt;p&gt;A strong front-end developer experience moves smoothly through each of these phases. Unfortunately, few front-end development experiences are seamless.&lt;/p&gt; &lt;h2&gt;Common pain points for front-end developers&lt;/h2&gt; &lt;p&gt;Common front-end development pain points span three main areas: environments, testing, and releases. Often, front-end development environments lack key features that end-user environments have, such as authorization or live data. Frequently, networking pieces are missing or need to be properly proxied to test these environments. And speaking of tests, they don't write, run, or analyze themselves!&lt;/p&gt; &lt;p&gt;There's no silver bullet that solves any of these problems, yet front-end developers spend a significant portion of their time on them. Let's examine each of these pain points in detail, and look at methods front-end developers and &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; engineers can use to help solve them.&lt;/p&gt; &lt;h3&gt;Pain point #1: Environments&lt;/h3&gt; &lt;p&gt;While most front ends start standalone, few continue on their own because they usually need certain back-end services. Back-end developers can develop without a front end, but front-end developers certainly can’t develop without a back end. In general, there are three solutions to running back-end services to use against a local front end:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Run the back-end services locally (see Figure 1).&lt;/li&gt; &lt;li&gt;Use a shared back-end deployment.&lt;/li&gt; &lt;li&gt;Use mock data in your front end.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Solutions one and three allow offline development. While an internet connection is a given nowadays, offline development is still a better experience than online development for developers who travel or live in places that experience intermittent outages due to weather. Offline development also doesn't require a VPN, which can be difficult to set up on certain devices.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/hot-reloading.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/hot-reloading.gif" width="2607" height="1387" alt="Text editor and browser open side-by-side" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Hot-reloading changes with Webpack on a local cloud.redhat.com environment with back-end services running locally.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Solutions one and two require an extra step to run a command before developing the front end. The local back end might need additional configuration (like a database), and a shared back end might require a proxy. This leads to a worse developer experience but can help catch back-end bugs earlier than mocking the data might.&lt;/p&gt; &lt;p&gt;The third solution provides the best overall developer experience but requires the most work, because each back-end endpoint must be spoofed to return mocked data. This requires front-end and back-end developers working together to create mocks.&lt;/p&gt; &lt;p&gt;The first solution provides the best back-end developer experience because they can test local back-end changes against a front end.&lt;/p&gt; &lt;p&gt;The environment is the first pain point that needs solving for front-end developers, and is generally the hardest of the common pain points to get right. Being able to work offline is a good experience. Not having to run a back end to make front end changes is a &lt;em&gt;great&lt;/em&gt; experience.&lt;/p&gt; &lt;h3&gt;Pain point #2: Testing&lt;/h3&gt; &lt;p&gt;Most front ends don't start out needing tests. However, as they grow, it's important that the user experience doesn't degrade when adding new features or fixing unrelated bugs. Writing, running, and reporting tests can be painful. Let's take a look at how to ease the testing process across two different front-end testing categories: unit tests and integration tests.&lt;/p&gt; &lt;h4&gt;Unit tests&lt;/h4&gt; &lt;p&gt;The best way to make test writing easier is only to write tests that are important. Adding automatic snapshot tests for custom components can help catch bugs. After that, spend time testing event and state interactions. When developing tests, most tools have a watch mode that can be used to run tests only when the test files change.&lt;/p&gt; &lt;h4&gt;Integration tests&lt;/h4&gt; &lt;p&gt;With certain frameworks, it’s possible to &lt;a href="https://chrome.google.com/webstore/detail/cypress-recorder/glcapdcacdfkokcmicllhcjigeodacab?hl=en-US"&gt;record user interactions&lt;/a&gt; on a webpage to avoid coding the test cases yourself. For large test suites, tools like &lt;a href="https://www.browserstack.com/"&gt;BrowserStack&lt;/a&gt; offer a grid of 10 concurrent runners for free and open source accounts.&lt;/p&gt; &lt;h4&gt;Reporting tests&lt;/h4&gt; &lt;p&gt;There are dozens of reporting formats, but HTML reports that can be uploaded to a pull request (PR) like the &lt;a href="https://github.com/patternfly/patternfly-react/pull/5524"&gt;one shown in Figure 2&lt;/a&gt; are usually best. For unit tests, tools like &lt;a href="http://codecov.io"&gt;Codecov&lt;/a&gt; can help maintain a certain coverage percentage as well.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/a11y-report.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/a11y-report.png?itok=4Rc7RPoX" width="600" height="345" alt="An HTML accessibility report for an @patternfly/react-core pull request " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: An HTML accessibility report for a pull request complete with screenshots.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Pain point #3: Releasing&lt;/h3&gt; &lt;p&gt;When opening a pull request, it’s often useful to share changes with designers and other developers in the form of a PR preview. If you just need to host static files, services like &lt;a href="https://www.netlify.com/"&gt;Netlify&lt;/a&gt; can work with minimal configuration, or a service like &lt;a href="https://surge.sh/"&gt;Surge&lt;/a&gt; can work with an existing continuous integration (CI) system. For sites that also need a back end, &lt;a href="https://vercel.com/"&gt;Vercel&lt;/a&gt; and &lt;a href="https://www.heroku.com/"&gt;Heroku&lt;/a&gt; have free tiers sufficient for most deployments.&lt;/p&gt; &lt;p&gt;For releasing, tools like &lt;a href="https://github.com/semantic-release/semantic-release"&gt;semantic-release&lt;/a&gt; for normal repos and &lt;a href="https://lerna.js.org/"&gt;Lerna&lt;/a&gt; for monorepos (see Figure 3) can release to Git, GitHub, or Node Package Manager (npm) on every push to the main branch of your repository. Of course, there's always the option to write your own bash script for complete flexibility.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/patternfly-npm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/patternfly-npm.png?itok=qIk-spJt" width="600" height="294" alt="Lerna auto-releasing @patternfly/react-* packages to npm from a merged pull request." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Lerna auto-releasing @patternfly/react-* packages to npm from a merged pull request.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion: Developer experience is user experience&lt;/h2&gt; &lt;p&gt;Improving the front-end developer experience lowers the cost of front-end development and enhances the overall user experience. Addressing front-end developer pain points will look different for every project, but the payoff is the same: A smoother experience for developers carries through to your end users.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience" title="How to create a better front-end developer experience "&gt;How to create a better front-end developer experience &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lVMpKj-n8rc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Zackary Allen</dc:creator><dc:date>2021-06-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience</feedburner:origLink></entry><entry><title type="html">Retail data framework - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xXFiSsZFv0A/retail-data-framework-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</id><updated>2021-06-01T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements  In our  from this series we introduced a use case around the data framework for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the real-time stock control architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architectural blueprint.  It's our intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. DATA INTEGRATION PLATFORM The logical view splits this solution space into several identifiable platforms where the retail data framework is managed. It takes all three of these platforms to ensure that the data is collected, integrated with the various blueprints external to the framework, processed, validated, stored, data science is applied for insights, and exposed back out to the entire retail organisation. The first we'll look at is the data integration platform where the main action takes place with the retail data framework. Here there are integration microservices and data integration microservices to provide integration with the core platform, data science platform, and storage services.  Another important set of elements found in this platform are messaging and event processing. Both are essential elements to ensure microservice communications and message transformation within the architecture. To help with data performance, availability, and management there are data caching microservices and data virtualisation microservices.  Next up, process automation is used to capture processes within the retail organisation, manage the processing and validation of data in a structured traceable manner. The business automation microservices capture all of these processes and ensure proper monitoring of the compliance and regulatory rules for the entire retail organisation.  Finally, ensuring that the fronting web applications have good visual data representations requires that all microservices are only accessed by authenticated and authorised parties. This is taken care of through the use of an API management element.  You can sense that this data integration platform contains the elements focusing on microservice deployments which lend themselves to a cloud-native development process using containers and container platforms.  CORE PLATFORM A core platform focusing on security and compliance requires the hosting of retail organisation wide tooling. These are not specifically called out and can be any number of core services or systems hosted within the retail organisation or outside in the form of Software as a Service (SaaS) solutions. This platform hosts a set of four elements that each support the organisation, starting with compliance and regulatory tooling. This is not the rules mentioned in the previous section, but the tooling backing the development, deployment, and maintenance of the compliance and regulatory rules.  There should be some form of auditing tooling and governance tooling used to ensure data and the services used to support the retail data framework are properly monitored in their application. Finally, the authentication and authorisation tooling is the central system used to plug in organisational wide access to the right parties within the retail data framework. DATA SCIENCE PLATFORM Any retail organisation working in their markets has a vast interest in the behaviour patterns of their customers. At the very least they are using the most basic data science elements, and in advanced cases, they are leveraging all forms of data science to advance their market positions. In the data science platform we find the more classical business intelligence tooling, often an externally hosted system noted here with a private cloud icon. Providing views and cuts of the mass data collected in retail organisations is the fundamental function of this element.  The advanced use of not just analytics on their data, but applying more sophisticated technologies like data science (AI / ML) allows retail organisations to gain advantageous insights into customers, trends, products, sales, and other retail activities that raw data analysis can't provide. Finally, there is data visualisation tooling that provides clear visibility to the consumers of the data and analysis generated from the other elements on this platform. STORAGE SERVICES The storage services uncovered in this solution space was a fairly diverse and more high level than the usually noted storage elements found in our architecture blueprints. As these storage needs are data focused and organisation wide solutions, you find all the major technologies in the data world applied here, such as data lakes, data warehousing, data hubs, and data marts. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the retail data framework use case.  An overview of this series on retail data framework portfolio architecture blueprint: 1. 2. 3. Example data architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example data framework architecture for this blueprint. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xXFiSsZFv0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</feedburner:origLink></entry></feed>
